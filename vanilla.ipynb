{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f86ddc-ac61-4716-b0f5-c2e7e7fa7c5e",
   "metadata": {},
   "source": [
    "Attention is All You Need (Vaswani et al., 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8a26ce-620a-4cfe-b863-a96a09569373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f4e959b-b6c3-4387-9505-cf8da7f195a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter-23524026/.local/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687dd475-505d-4720-9c64-be78db2ae0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d019e-c3e8-4a62-80b2-72b030eb5554",
   "metadata": {},
   "source": [
    "Load dataset, split train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca55ae64-4bbe-4f89-9c79-034e438666c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sciq\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "val_data = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09a764-b45f-4b6e-aab1-2b361e92d839",
   "metadata": {},
   "source": [
    "Tokenization, padding, truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0c5779-1445-45e2-a110-69f36c1f554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "MAX_INPUT_LEN = 64\n",
    "MAX_TARGET_LEN = 16\n",
    "\n",
    "def preprocess(example):\n",
    "    input_text = f\"question: {example['question']}\"\n",
    "    target_text = example[\"correct_answer\"]\n",
    "\n",
    "    input_enc = tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LEN\n",
    "    )\n",
    "    target_enc = tokenizer(\n",
    "        target_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_TARGET_LEN\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_enc.input_ids),\n",
    "        \"attention_mask\": torch.tensor(input_enc.attention_mask),\n",
    "        \"labels\": torch.tensor(target_enc.input_ids),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a9537-1c69-4f0c-9dab-4589407eff1a",
   "metadata": {},
   "source": [
    "PyTorch dataset wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fa0d08-bfb7-4236-8060-e1c996ada6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciQDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.data = [preprocess(x) for x in hf_dataset]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabb704-8eda-46ce-8984-5192e34859f6",
   "metadata": {},
   "source": [
    "Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b79a73-9108-42db-8d6a-a421715e3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SciQDataset(train_data)\n",
    "val_dataset = SciQDataset(val_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8192dc-a2da-4362-ae55-ab2a57bdbcae",
   "metadata": {},
   "source": [
    "Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0749f98e-2b25-4d26-9736-01a0ba94b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=64):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0c981-de7c-43b7-b80b-6bdbe3d5add5",
   "metadata": {},
   "source": [
    "Masking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3fb9c8-b8a1-40bc-bc87-e8149ba0b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_causal_mask(size):\n",
    "    mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f705eb5-5298-415e-af85-2db0a01b3389",
   "metadata": {},
   "source": [
    "Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5284720-5910-444a-9da6-a3712c091c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, mask=None, kv=None):\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "\n",
    "        B, T_q, _ = x.size()\n",
    "        B, T_kv, _ = kv.size()\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(kv)\n",
    "        v = self.v_proj(kv)\n",
    "\n",
    "        q = q.view(B, T_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(B, T_q, -1)\n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82d44c-4ec1-4d19-a8fb-78b03b4ba12d",
   "metadata": {},
   "source": [
    "Feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fba20fe-fd78-4e2f-8e53-d5cfa765256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf566d-114b-4188-b296-f59d283813d2",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef71254b-6260-441c-8b77-34a83335f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eca637-de94-4a5e-b440-395c5a17c3e6",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "418b115c-f4b3-47b9-a918-dc1001f42f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        causal_mask = generate_causal_mask(x.size(1)).to(x.device)\n",
    "        x = x + self.self_attn(self.norm1(x), mask=causal_mask)\n",
    "        x = x + self.cross_attn(self.norm2(x), kv=enc_out)\n",
    "        x = x + self.ff(self.norm3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f80441-f73c-4619-aabd-f3b655787531",
   "metadata": {},
   "source": [
    "Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ceb84f-6e70-4e3d-9b57-f83a93e712a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, num_heads=2, num_layers=2, d_ff=128, max_len=64):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.dropout(self.pos_enc(self.tok_emb(src)))\n",
    "        tgt = self.dropout(self.pos_enc(self.tok_emb(tgt)))\n",
    "        for layer in self.enc_layers:\n",
    "            src = layer(src)\n",
    "        for layer in self.dec_layers:\n",
    "            tgt = layer(tgt, src)\n",
    "        return self.out(tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d10d5-3a4e-4ab6-ab9b-12acc7370c30",
   "metadata": {},
   "source": [
    "Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "411e9062-bf20-492e-b180-6ae62c405292",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = Transformer(vocab_size=vocab_size).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca6d909c-14a5-4f84-84d0-4f604bb9f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            output = model(input_ids, labels[:, :-1])\n",
    "            logits = output[:, :, :]\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels[:, 1:].reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46d414-3a82-4b75-8231-a6cfe79447cd",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8fe30dc-1a3b-431f-b79f-9734a490aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 730/730 [00:57<00:00, 12.76it/s, train_loss=5.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 | Train Loss: 6.4039 | Val Loss: 5.5893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 730/730 [00:52<00:00, 13.95it/s, train_loss=5.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 | Train Loss: 5.4183 | Val Loss: 5.4089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 730/730 [00:55<00:00, 13.10it/s, train_loss=5.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 | Train Loss: 5.2564 | Val Loss: 5.2817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 730/730 [00:53<00:00, 13.59it/s, train_loss=5.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 | Train Loss: 5.1243 | Val Loss: 5.1946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 730/730 [00:53<00:00, 13.72it/s, train_loss=5.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 | Train Loss: 4.9995 | Val Loss: 5.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 730/730 [00:55<00:00, 13.22it/s, train_loss=4.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 | Train Loss: 4.8850 | Val Loss: 4.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 730/730 [00:54<00:00, 13.37it/s, train_loss=4.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 | Train Loss: 4.7773 | Val Loss: 4.9072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 730/730 [00:53<00:00, 13.60it/s, train_loss=5.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 | Train Loss: 4.6739 | Val Loss: 4.8334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 730/730 [00:55<00:00, 13.10it/s, train_loss=4.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 | Train Loss: 4.5794 | Val Loss: 4.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 730/730 [00:56<00:00, 13.01it/s, train_loss=5.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 | Train Loss: 4.4866 | Val Loss: 4.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 730/730 [00:56<00:00, 12.99it/s, train_loss=3.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 | Train Loss: 4.3991 | Val Loss: 4.6524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 730/730 [00:54<00:00, 13.49it/s, train_loss=4.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 | Train Loss: 4.3290 | Val Loss: 4.6096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 730/730 [00:55<00:00, 13.12it/s, train_loss=3.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 | Train Loss: 4.2583 | Val Loss: 4.5515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 730/730 [00:56<00:00, 12.95it/s, train_loss=4.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 | Train Loss: 4.1974 | Val Loss: 4.5359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 730/730 [00:56<00:00, 12.87it/s, train_loss=3.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 | Train Loss: 4.1369 | Val Loss: 4.4928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 730/730 [00:54<00:00, 13.35it/s, train_loss=4.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 | Train Loss: 4.0749 | Val Loss: 4.4621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 730/730 [01:02<00:00, 11.71it/s, train_loss=4.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 | Train Loss: 4.0197 | Val Loss: 4.4349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 730/730 [01:04<00:00, 11.31it/s, train_loss=4.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 | Train Loss: 3.9747 | Val Loss: 4.4089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 730/730 [01:05<00:00, 11.09it/s, train_loss=4.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 | Train Loss: 3.9279 | Val Loss: 4.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 730/730 [01:05<00:00, 11.13it/s, train_loss=3.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 | Train Loss: 3.8744 | Val Loss: 4.3800\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        decoder_input = labels[:, :-1]\n",
    "        decoder_target = labels[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, decoder_input)\n",
    "        loss = loss_fn(output.view(-1, output.size(-1)), decoder_target.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(train_loss=loss.item())\n",
    "\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fce84ecf-2470-4011-bc38-69b97bf7b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            decoder_input = labels[:, :-1]\n",
    "            target = labels[:, 1:]\n",
    "\n",
    "            output = model(input_ids, decoder_input)\n",
    "            pred_ids = output.argmax(dim=-1)\n",
    "            correct += (pred_ids == target).float().sum().item()\n",
    "            total += target.numel()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b623d25d-cbcb-4596-bda0-5e7965d44278",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"transformer_qa.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b07810d-c8b0-45b2-8b88-a5275b61d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"transformer_qa.pt\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e958d2b-203e-4060-b535-822a97e9db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question_text, max_len=MAX_TARGET_LEN):\n",
    "    model.eval()\n",
    "    input_text = f\"question: {question_text}\"\n",
    "    enc = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_INPUT_LEN\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "\n",
    "    decoder_input = torch.full((1, 1), tokenizer.pad_token_id, dtype=torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, decoder_input)\n",
    "\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token = next_token_logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(decoder_input[0][1:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1736d22b-5714-4574-a490-390a60df3a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Compounds that are capable of accepting electrons, such as o 2 or f2, are called what?\n",
      "Model: \n",
      "True: oxidants\n",
      "\n",
      "Q: What term in biotechnology means a genetically exact copy of an organism?\n",
      "Model: \n",
      "True: clone\n",
      "\n",
      "Q: Vertebrata are characterized by the presence of what?\n",
      "Model: \n",
      "True: backbone\n",
      "\n",
      "Q: What is the height above or below sea level called?\n",
      "Model: \n",
      "True: elevation\n",
      "\n",
      "Q: Ice cores, varves and what else indicate the environmental conditions at the time of their creation?\n",
      "Model: \n",
      "True: tree rings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(5):\n",
    "    q = test_data[i][\"question\"]\n",
    "    a = generate_answer(q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"Model: {a}\")\n",
    "    print(f\"True: {test_data[i]['correct_answer']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74f671-e3b2-4b12-9c11-8d373824f156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "venv_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
